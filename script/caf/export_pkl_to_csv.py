#!/usr/bin/env python3
"""
PKLæ–‡ä»¶å¯¼å‡ºä¸ºCSVè„šæœ¬

ç”¨äºå°†semantic_memoryä¸­çš„pklæ–‡ä»¶å¯¼å‡ºä¸ºCSVæ ¼å¼ï¼Œæ–¹ä¾¿æŸ¥çœ‹å’Œåˆ†æ

Usage:
    python scripts/export_pkl_to_csv.py [--database DATABASE_ID] [--type METADATA_TYPE] [--path FILE_PATH] [--output OUTPUT_PATH]

Examples:
    # å¯¼å‡ºcalifornia_schoolsæ•°æ®åº“çš„column metadataä¸ºCSV
    python script/caf/export_pkl_to_csv.py --database california_schools --type term
    
    # å¯¼å‡ºæŒ‡å®šæ–‡ä»¶
    python scripts/export_pkl_to_csv.py --path /home/yangchenyu/Text2SQL/memory/semantic_memory/california_schools/column.pkl
    
    # æŒ‡å®šè¾“å‡ºè·¯å¾„
    python scripts/export_pkl_to_csv.py --database california_schools --type column --output output/columns.csv
    
    # å¯¼å‡ºæ‰€æœ‰æ•°æ®åº“çš„æ‰€æœ‰æ–‡ä»¶
    python scripts/export_pkl_to_csv.py --export-all --output-dir output/
"""

import argparse
import pandas as pd
import sys
from pathlib import Path
from typing import Optional, List, Tuple
import json

def export_pkl_to_csv(file_path: Path, output_path: Optional[Path] = None, 
                     include_index: bool = False, encoding: str = 'utf-8'):
    """å°†pklæ–‡ä»¶å¯¼å‡ºä¸ºCSV"""
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    try:
        # åŠ è½½DataFrame
        df = pd.read_pickle(file_path)
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_path is None:
            output_path = file_path.with_suffix('.csv')
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å¯¼å‡ºä¸ºCSV
        df.to_csv(output_path, index=include_index, encoding=encoding)
        
        print(f"âœ… æˆåŠŸå¯¼å‡º: {file_path} -> {output_path}")
        print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.2f} KB")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return False

def export_all_pkl_files(semantic_memory_dir: Path, output_dir: Path, 
                         include_index: bool = False, encoding: str = 'utf-8'):
    """å¯¼å‡ºæ‰€æœ‰pklæ–‡ä»¶ä¸ºCSV"""
    if not semantic_memory_dir.exists():
        print("âŒ semantic_memoryç›®å½•ä¸å­˜åœ¨")
        return False
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    
    exported_count = 0
    failed_count = 0
    
    print(f"ğŸ—‚ï¸  å¼€å§‹å¯¼å‡ºæ‰€æœ‰pklæ–‡ä»¶åˆ°: {output_dir}")
    print("=" * 60)
    
    # éå†æ‰€æœ‰æ•°æ®åº“ç›®å½•
    for db_dir in semantic_memory_dir.iterdir():
        if not db_dir.is_dir():
            continue
            
        print(f"\nğŸ“ å¤„ç†æ•°æ®åº“: {db_dir.name}")
        
        # åˆ›å»ºæ•°æ®åº“å­ç›®å½•
        db_output_dir = output_dir / db_dir.name
        db_output_dir.mkdir(exist_ok=True)
        
        # å¤„ç†è¯¥æ•°æ®åº“ä¸‹çš„æ‰€æœ‰pklæ–‡ä»¶
        pkl_files = list(db_dir.glob("*.pkl"))
        for pkl_file in pkl_files:
            output_path = db_output_dir / f"{pkl_file.stem}.csv"
            
            if export_pkl_to_csv(pkl_file, output_path, include_index, encoding):
                exported_count += 1
            else:
                failed_count += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š å¯¼å‡ºå®Œæˆ:")
    print(f"  âœ… æˆåŠŸ: {exported_count} ä¸ªæ–‡ä»¶")
    print(f"  âŒ å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
    
    return exported_count > 0

def create_summary_report(semantic_memory_dir: Path, output_dir: Path):
    """åˆ›å»ºæ•°æ®æ‘˜è¦æŠ¥å‘Š"""
    summary_data = {
        "databases": {},
        "total_files": 0,
        "total_rows": 0,
        "export_timestamp": pd.Timestamp.now().isoformat()
    }
    
    for db_dir in semantic_memory_dir.iterdir():
        if not db_dir.is_dir():
            continue
            
        db_name = db_dir.name
        db_data = {
            "files": {},
            "total_rows": 0
        }
        
        pkl_files = list(db_dir.glob("*.pkl"))
        for pkl_file in pkl_files:
            try:
                df = pd.read_pickle(pkl_file)
                file_info = {
                    "rows": df.shape[0],
                    "columns": df.shape[1],
                    "file_size_kb": pkl_file.stat().st_size / 1024,
                    "column_names": list(df.columns)
                }
                db_data["files"][pkl_file.stem] = file_info
                db_data["total_rows"] += df.shape[0]
                summary_data["total_rows"] += df.shape[0]
            except Exception as e:
                db_data["files"][pkl_file.stem] = {"error": str(e)}
        
        summary_data["databases"][db_name] = db_data
        summary_data["total_files"] += len(pkl_files)
    
    # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
    summary_path = output_dir / "export_summary.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“‹ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
    return summary_data

def main():
    parser = argparse.ArgumentParser(description="å°†semantic_memoryä¸­çš„pklæ–‡ä»¶å¯¼å‡ºä¸ºCSVæ ¼å¼")
    parser.add_argument(
        '--database', '-d',
        type=str,
        help="æ•°æ®åº“ID (ä¾‹å¦‚: california_schools)"
    )
    parser.add_argument(
        '--type', '-t',
        type=str,
        choices=['database', 'table', 'column', 'relationship', 'term'],
        help="metadataç±»å‹"
    )
    parser.add_argument(
        '--path', '-p',
        type=Path,
        help="ç›´æ¥æŒ‡å®špklæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        '--output', '-o',
        type=Path,
        help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        '--output-dir',
        type=Path,
        help="æ‰¹é‡å¯¼å‡ºæ—¶çš„è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        '--export-all',
        action='store_true',
        help="å¯¼å‡ºæ‰€æœ‰æ•°æ®åº“çš„æ‰€æœ‰pklæ–‡ä»¶"
    )
    parser.add_argument(
        '--include-index',
        action='store_true',
        help="åœ¨CSVä¸­åŒ…å«è¡Œç´¢å¼•"
    )
    parser.add_argument(
        '--encoding',
        type=str,
        default='utf-8',
        help="CSVæ–‡ä»¶ç¼–ç  (é»˜è®¤: utf-8)"
    )
    parser.add_argument(
        '--create-summary',
        action='store_true',
        help="åˆ›å»ºæ•°æ®æ‘˜è¦æŠ¥å‘Š"
    )
    parser.add_argument(
        '--semantic-dir',
        type=Path,
        default=Path(__file__).parent.parent.parent / "memory" / "semantic_memory",
        help="semantic_memoryç›®å½•è·¯å¾„ (é»˜è®¤: ./memory/semantic_memory)"
    )
    
    args = parser.parse_args()
    
    # å¯¼å‡ºæ‰€æœ‰æ–‡ä»¶
    if args.export_all:
        output_dir = args.output_dir or Path("output")
        success = export_all_pkl_files(args.semantic_dir, output_dir, 
                                     args.include_index, args.encoding)
        
        if args.create_summary:
            create_summary_report(args.semantic_dir, output_dir)
        
        if not success:
            sys.exit(1)
        return
    
    # ç¡®å®šè¦å¯¼å‡ºçš„æ–‡ä»¶
    file_path = None
    
    if args.path:
        # ç›´æ¥æŒ‡å®šæ–‡ä»¶è·¯å¾„
        file_path = args.path
    elif args.database and args.type:
        # é€šè¿‡æ•°æ®åº“IDå’Œç±»å‹æŒ‡å®š
        file_path = args.semantic_dir / args.database / f"{args.type}.pkl"
    else:
        print("âŒ è¯·æŒ‡å®šè¦å¯¼å‡ºçš„æ–‡ä»¶:")
        print("  æ–¹å¼1: --path /path/to/file.pkl")
        print("  æ–¹å¼2: --database DATABASE_ID --type METADATA_TYPE")
        print("  æ–¹å¼3: --export-all (å¯¼å‡ºæ‰€æœ‰æ–‡ä»¶)")
        sys.exit(1)
    
    # å¯¼å‡ºæ–‡ä»¶
    success = export_pkl_to_csv(file_path, args.output, 
                               args.include_index, args.encoding)
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    main()
